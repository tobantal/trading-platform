# Лекция: Микросервисная архитектура — от теории к практике

## Блок 1: Технологический стек и основные компоненты

### 1.1 Обзор основных технологий курса

Курс "Microservice Architecture" охватывает полный спектр инструментов и паттернов для разработки современных распределённых систем. Все технологии, рассматриваемые в курсе, будут ожидаться преподавателями на защите проекта.

#### 1.1.1 Основные модули курса

**Модуль 1: Базовые паттерны микросервисной архитектуры**

Этот модуль закладывает фундамент. Здесь изучаются:

- **Монолит vs микросервисы**: плюсы монолитной архитектуры (простота разработки, единая БД) и её боли (масштабирование, развёртывание, независимое развитие команд). Преимущества микросервисов (масштабируемость, независимость, гибкость) и их сложность.

- **Паттерн Strangler**: использование для постепенной миграции монолита в микросервисы без полной переписки системы. Позволяет параллельно развивать старую и новую архитектуру.

- **Паттерны декомпозиции**: разбиение системы на сервисы по пользовательским сценариям, модели предметной области (DDD), функциональным возможностям. Здесь ключевой вклад Domain-Driven Design (DDD) — выделение Bounded Contexts.

- **Аутентификация и авторизация**: от простых подходов в монолитах к сложным схемам в микросервисах. Включает JWT токены, Identity Provider-ы (OIDC), auth proxy, межсервисную аутентификацию.

**Модуль 2: Инфраструктура микросервисов**

Инфраструктура — это кровеносная система микросервисов:

- **CI/CD пайплайны**: автоматизация сборки, тестирования и развёртывания. Важно понимать разницу между VM, серверами приложений и контейнерами.

- **Docker**: контейнеризация приложений. Ключевые команды (build, run, push, pull), организация Dockerfile, оптимизация размера образов.

- **Kubernetes**: оркестрация контейнеров. Архитектура (master/worker ноды), базовые сущности:
  - **Pod** — минимальная единица, может содержать несколько контейнеров
  - **Deployment** — управляет репликацией подов, обновления версий
  - **Service** — сетевая абстракция, маршрутизация трафика
  - **Ingress** — маршрутизация HTTP трафика извне кластера
  - **Persistent Volume** и **Persistent Volume Claim** — управление хранилищем
  - **StatefulSet** — для stateful приложений (БД)
  - **ConfigMap** и **Secret** — конфигурация и чувствительные данные
  - **Job** и **CronJob** — выполнение задач один раз или по расписанию
  - **DaemonSet** — запуск подов на каждой ноде

- **Helm**: управление Kubernetes манифестами через шаблоны. Позволяет переиспользовать конфигурации.

- **Service Discovery**: автоматическое обнаружение адресов сервисов в сети. В Kubernetes встроено (DNS имена сервисов).

- **Health Checks**: liveness и readiness пробы — механизмы проверки здоровья приложения.

**Модуль 3: Инструменты наблюдаемости**

Observability — это visibility inside the black box:

- **Мониторинг и алертинг**: сбор метрик, определение SLI/SLO/SLA, бюджет на ошибки (error budget), управление инцидентами. Принципы: USE (Utilization, Saturation, Errors) для инфраструктуры, RED (Rate, Errors, Duration) для приложений, Four Golden Signals.

- **Prometheus**: система сбора и хранения временных рядов (time-series) метрик. PromQL — язык запросов для аналитики. Service Monitors — автоматическое обнаружение целей для мониторинга.

- **Grafana**: визуализация метрик Prometheus, создание дашбордов, управление алертами.

- **Логирование (ELK, EFK, Graylog)**: агрегация логов из разных сервисов. Стек ELK (Elasticsearch, Logstash, Kibana) или EFK (Elasticsearch, Fluent/Fluentd, Kibana). Паттерн структурированного логирования.

- **Распределённая трассировка**: отслеживание одного запроса через все микросервисы. OpenTelemetry — стандарт для инструментирования. OpenTracing — старый стандарт, сейчас вытесняется OpenTelemetry.

**Модуль 4: Коммуникационные паттерны**

Как микросервисы общаются между собой:

- **API Gateway и BFF (Backend for Frontend)**: единая точка входа для клиентов. API Gateway обрабатывает аутентификацию, rate limiting, маршрутизацию. BFF — отдельный сервис для каждого типа клиента (web, mobile, desktop) с оптимизацией под их потребности.

- **Circuit Breaker и Retry**: паттерны отказоустойчивости. Circuit Breaker разрывает цепь при частых ошибках (fail-fast), Retry с экспоненциальной задержкой для временных сбоев.

- **Синхронный API**: REST, GraphQL, gRPC. REST (Level 3 Maturity Model) — наиболее распространён. GraphQL — гибкость для клиентов. gRPC — высокая производительность, протокол протобуф.

- **Асинхронный API**: события (events) и сообщения (messages). Оркестрация vs хореография. Версионирование API, OpenAPI/Swagger для описания.

- **Event-Driven Architecture**: система на основе событий. События как source of truth. Паттерны проектирования событий: domain events, integration events.

- **Message Brokers**: RabbitMQ (AMQP), Kafka. RabbitMQ — традиционная система очередей, Kafka — distributed log, лучше для Event Sourcing.

- **Service Mesh (Istio)**: управление трафиком между микросервисами на уровне инфраструктуры. Позволяет реализовать circuit breaking, retries, rate limiting без изменения кода приложения.

**Модуль 5: Распределённые системы и хранилища**

Самая сложная часть — консистентность данных в распределённой среде:

- **CAP теорема**: при сетевом разделении нельзя одновременно гарантировать Consistency и Availability. PACELC теорема уточняет: что выбирать между Latency и Consistency когда нет разделений.

- **ACID vs BASE**: ACID (Atomicity, Consistency, Isolation, Durability) для монолитных БД, BASE (Basically Available, Soft state, Eventually consistent) для распределённых.

- **Распределённые транзакции**: двухфазный коммит (2PC) — блокирует ресурсы (плохо масштабируется), паттерн Saga (Orchestration vs Choreography) — разбиение большой транзакции на цепь локальных с компенсациями при откате. Outbox Pattern для надёжной публикации событий.

- **Кэширование**: паттерны (Cache-aside, Write-through, Write-behind), инструменты (Redis для distributed cache, LRU/LFU алгоритмы).

- **Шардирование**: разделение данных по некоему ключу (например, по user ID). Стратегии (range-based, hash-based, directory-based, consistent hashing). Challenges: перебалансировка, горячие шарды.

- **Консистентность данных**: CP системы (Paxos, Raft, Zab) гарантируют консистентность но требуют синхронизации. AP системы (gossip protocols, Dynamo-подобные БД) обменяются на консистентность ради доступности.

- **Stream Processing**: Event Sourcing (все изменения как события), Change Data Capture (CDC) — отслеживание изменений в БД.

- **Идемпотентность и коммутативность**: безопасное повторение операций (особенно в очередях).

#### 1.1.2 Ожидаемые технологии на защите

Преподаватели будут ожидать, что вы:

1. **Можете объяснить архитектурные выборы**: почему выбрали именно эту технологию (например, RabbitMQ вместо Kafka)
2. **Понимаете паттерны**: как реализован Circuit Breaker, SAGA, Event Sourcing в вашем проекте
3. **Работали с инструментами**: Docker, Kubernetes, Prometheus, Grafana (хотя бы базово)
4. **Решали real-world проблемы**: как обеспечить консистентность, как масштабировать, как отлаживать

---

### 1.2 Практическое применение в проекте торговой платформы

#### 1.2.1 Декомпозиция на микросервисы

Trading Platform должна быть разделена на логические модули (Bounded Contexts):

**Identity Context** — управление аутентификацией и сессиями
- Входящий порт: `IAuthService`
- Выходящие порты: `IUserRepository`, `IBrokerGateway` (для валидации токена), `ICache`
- Отвечает за: JWT токены, управление сессиями, привязка брокерских счётов к пользователям

**Market Data Context** — получение котировок и данных по инструментам
- Входящий порт: `IMarketDataService`
- Выходящие порты: `IBrokerGateway`, `ICache`
- Отвечает за: real-time котировки, поиск инструментов, кэширование

**Trading Context** — создание и управление ордерами
- Входящий порт: `IOrderService`
- Выходящие порты: `IBrokerGateway`, `IOrderRepository`, `IEventPublisher`
- Отвечает за: валидация ордеров, отправка брокеру, хранение, публикация событий

**Portfolio Context** — позиции, P&L, денежные средства
- Входящий порт: `IPortfolioService`
- Выходящие порты: `IBrokerGateway`, `IPositionRepository`
- Отвечает за: расчёт стоимости, P&L, история операций

**Strategy Context** — алгоритмические торговые стратегии
- Входящий порт: `IStrategyService`
- Выходящие порты: `IOrderService`, `IMarketDataService`, `IStrategyRepository`, `IEventPublisher`
- Отвечает за: регистрация стратегий, запуск/остановка, обработка сигналов

**Notification Context** — WebSocket уведомления для UI
- Входящий порт: `INotificationService`
- Выходящие порты: слушает события из других модулей
- Отвечает за: real-time уведомления пользователю

#### 1.2.2 Взаимодействие модулей

В MVP (monolithic) версии:
- Прямые вызовы через интерфейсы (ports/adapters)
- RabbitMQ для асинхронных событий (order.created → Notification)

При развитии в микросервисы:
- REST API между сервисами для синхронных операций
- gRPC для высоконагруженных операций (market data)
- RabbitMQ для асинхронных событий

#### 1.2.3 Данные и консистентность

**Проблема**: когда пользователь создаёт ордер, нужно:
1. Сохранить в БД (Trading Context)
2. Отправить брокеру (Tinkoff Gateway)
3. Опубликовать событие (для Strategy и Notification)

**Решение (Saga Pattern)**:
```
1. OrderService.createOrder() → создаёт заказ в статусе PENDING
2. Публикует OrderCreated event
3. BrokerGateway получает event и отправляет брокеру
4. При успехе: OrderConfirmed event
5. При ошибке: компенсирующая транзакция (отмена заказа)
```

**Outbox Pattern**: при сохранении заказа одновременно пишем в таблицу `outbox`:
```sql
BEGIN;
  INSERT INTO orders (...) VALUES (...);
  INSERT INTO outbox (event_type, payload) VALUES ('OrderCreated', ...);
COMMIT;

-- Отдельный процесс читает outbox и публикует события
```

#### 1.2.4 Мониторинг Trading Platform

**Метрики (Prometheus)**:
- `trading_orders_total` — количество созданных ордеров
- `trading_order_duration_ms` — время создания ордера
- `trading_strategy_pnl` — P&L каждой стратегии
- `broker_api_latency_ms` — задержка при обращении к Tinkoff
- `broker_api_errors_total` — количество ошибок при обращении к брокеру

**Логирование**:
- Структурированные логи в JSON формате
- Correlation ID для отслеживания одного запроса через все сервисы
- Уровни: DEBUG (для разработки), INFO (нормальная работа), WARN (потенциальная проблема), ERROR (ошибка)

**Алертинг**:
- Критично: нет подключения к брокеру
- Важно: большое количество ошибок при создании ордеров
- Информационно: стратегия долго не генерирует сигналы

---

## Блок 2: Требования к защите проекта

### 2.1 Общие требования к функциональности

На защите проект должен демонстрировать **полный пользовательский путь** от начала до конца. Это означает, что функционал должен быть целостным — не отдельные фрагменты, а готовые сценарии.

#### Критичные пользовательские сценарии (MVP)

**Сценарий 1: Подключение и просмотр портфеля**
- Вход через токен Tinkoff Invest API
- Выбор счёта (для тех, у кого несколько)
- Отображение позиций, P&L, доступных средств

**Сценарий 2: Торговля вручную**
- Поиск инструмента (по тикеру)
- Создание лимитного ордера (buy/sell)
- Просмотр списка активных ордеров
- Отмена ордера

**Сценарий 3: Автоматическая торговля (стратегия)**
- Выбор стратегии (например, SMA crossover)
- Настройка параметров
- Запуск стратегии
- Просмотр сигналов (buy/sell)
- Остановка стратегии

#### Критерии приёмки функционала

| Критерий | Что проверяется |
|----------|-----------------|
| **Работоспособность** | Все операции завершаются успешно без критических ошибок |
| **Пользовательский опыт** | Интерфейс интуитивный, ошибки понятны |
| **Целостность** | Каждый сценарий работает от начала до конца |
| **Демонстрируемость** | Можно показать в Postman/UI без заранее написанных скриптов |

### 2.2 Архитектурные требования

#### 2.2.1 Декомпозиция сервисов

**Вопрос на защите**: "Как вы декомпозировали сервисы?"

**Ожидаемый ответ должен включать**:
- Выявленные Bounded Contexts (DDD подход)
- Причина разделения (каждый контекст — отдельная область ответственности)
- Взаимодействие между контекстами
- Потенциальное развитие в микросервисы (как будут разделены в продакшене)

**Пример плохого ответа**: "Я создал сервис OrderService и всё остальное"

**Пример хорошего ответа**: "Я выделил 5 контекстов: Identity (управление доступом), Market (котировки), Trading (ордера), Portfolio (позиции), Strategy (стратегии). Каждый имеет свои порты (интерфейсы) и адаптеры. На защите это монолит с прямыми вызовами, но в production можно развернуть как микросервисы через REST API между ними."

#### 2.2.2 Мониторинг и алертинг

**Вопрос**: "Как устроены мониторинг и алертинг? Что вы мониторите и зачем?"

**Ожидаемо**:
- Определены ключевые метрики (не просто собираются все подряд)
- Метрики связаны с бизнес-целями (например, успешность создания ордеров)
- Настроены алерты на критичные события
- Dashboards в Grafana показывают здоровье системы

**Минимум**:
- Метрики latency, throughput, error rate для основных операций
- Метрики здоровья сервиса (CPU, Memory, Disk если stateful)
- Метрика подключения к внешним системам (Tinkoff API availability)

#### 2.2.3 Паттерны межсервисного взаимодействия

**Вопрос**: "Какие используются паттерны межсервисного взаимодействия?"

**Синхронное взаимодействие**:
- REST API между сервисами (в микросервисной версии)
- gRPC для высоконагруженных операций
- Синхронный паттерн: сервис A → сервис B, ждёт ответа

**Асинхронное взаимодействие**:
- Event-driven (события публикуются в message broker)
- Сервис не ждёт ответа, продолжает работу
- Пример: OrderCreated event → подписываются Strategy и Notification

**На защите должны быть примеры обоих**. Минимум:
- REST для создания ордера (синхронно)
- RabbitMQ для уведомлений пользователю об исполнении (асинхронно)

#### 2.2.4 Протоколы

**Вопрос**: "Какие протоколы используются?"

**Ожидаемый ответ**:
- HTTP/REST для API Gateway → UI
- HTTP/REST между микросервисами (если развёрнуты отдельно)
- gRPC к Tinkoff API (для получения котировок и создания ордеров)
- AMQP (RabbitMQ) для асинхронных событий
- WebSocket для real-time котировок в UI (опционально)

#### 2.2.5 Аутентификация и авторизация

**Вопрос**: "Как устроена аутентификация и авторизация?"

**Ожидаемо**:
- JWT токены для пользователей (выдаются после входа)
- Токены Tinkoff Invest API хранятся зашифрованными в БД
- Проверка JWT на входе в API Gateway
- Распространение контекста пользователя (user ID) в микросервисы

**Минимум**:
- JWT при входе с токеном Tinkoff
- Проверка JWT в контроллерах
- Зашифрованное хранилище для чувствительных данных

#### 2.2.6 API Gateway и Service Mesh

**Вопрос**: "Используется ли API Gateway или Service Mesh?"

**На MVP защиту**:
- API Gateway: да, он же сервер (Boost.Asio на C++) или любой другой
  - Функции: маршрутизация, JWT валидация, rate limiting
  - Фичи: Circuit Breaker, Retry для отказоустойчивости

**Service Mesh**:
- На MVP не обязателен (усложняет)
- Можно упомянуть как future: Istio для управления трафиком в продакшене

#### 2.2.7 Нагрузочное тестирование

**Вопрос**: "Проводили ли нагрузочное тестирование?"

**Для MVP**:
- Минимум: вручную проверили несколько ордеров одновременно
- Лучше: простой скрипт на Apache JMeter или wrk, показывающий, что система выдерживает нагрузку
- Ещё лучше: определены bottlenecks и как их решить

#### 2.2.8 Асинхронное взаимодействие и message broker

**Вопрос**: "Используется ли асинхронное взаимодействие? Какой брокер и почему?"

**Ожидаемо**:
- RabbitMQ или Kafka (почти всегда RabbitMQ для традиционных сценариев)
- **Почему RabbitMQ**: простой в использовании, хорошие гарантии доставки, достаточен для нашего случая
- **Когда использовать RabbitMQ**: события (order.created), уведомления пользователю
- **Когда Kafka**: если нужна история всех событий (Event Sourcing), high throughput

**На защите хотя бы один асинхронный путь**:
- Пример: создание ордера публикует OrderCreated, который слушает Notification сервис и отправляет уведомление в WebSocket

#### 2.2.9 Базы данных

**Вопрос**: "Какие БД используете и почему?"

**Выбор**:
- PostgreSQL основная (ACID гарантии, реляционная, стабильная)
- Redis для кэша и сессий (быстрый key-value store)
- Можно Elasticsearch если логирование

**На защиту должна быть схема БД** с таблицами для orders, strategies, positions

#### 2.2.10 Консистентность данных

**Вопрос**: "Как обеспечивается консистентность данных между разными сервисами?"

**Ключевой момент**: в распределённой системе нельзя гарантировать perfect consistency, только eventual consistency.

**Решения на выбор**:
- **SAGA паттерн** для распределённых транзакций (когда создание ордера затрагивает несколько сервисов)
- **Outbox Pattern** для надёжной публикации событий
- **Event Sourcing** если нужна полная история (future)

**На MVP минимум**: продумать, что будет если Tinkoff API упадёт после создания ордера в БД (правильный ответ: SAGA с компенсацией)

#### 2.2.11 Шардирование

**Вопрос**: "Используется ли шардирование?"

**На MVP**: нет, одна БД достаточна. Но нужно упомянуть, как бы это выглядело в production (например, по user ID если много пользователей).

#### 2.2.12 Кэширование

**Вопрос**: "Используется ли кэширование? Какая технология?"

**Ожидаемо**:
- Redis для распределённого кэша (session tokens, user context)
- Custom LRU кэш (из курса Алгоритмы) для hot data (последние котировки)
- Кэш котировок — критично, меняется часто, не может быть долгоживущим

**Паттерны кэширования**:
- Cache-aside: приложение проверяет кэш, если miss → идёт в БД
- Write-through: при обновлении пишем и в кэш, и в БД
- Важно: предусмотреть инвалидацию кэша

#### 2.2.13 Распределённые транзакции

**Вопрос**: "Есть ли распределённые транзакции? Как реализованы?"

**На MVP пример**:
- Создание ордера: нужно записать в БД AND отправить Tinkoff AND опубликовать событие
- **Неправильно**: писать в БД, потом отправлять Tinkoff, потом публиковать (если Tinkoff упадёт, событие никто не узнает)
- **Правильно**: SAGA паттерн или Outbox Pattern

---

### 2.3 Демонстрация на защите

#### Формат защиты (7-10 минут)

1. **Контекст (1 мин)**
   - Что это за система (торговая платформа)
   - Основные пользовательские сценарии

2. **Используемые технологии (1 мин)**
   - Язык (C++)
   - Frameworks (Boost.Asio, gRPC)
   - БД (PostgreSQL)
   - Инструменты (Docker, Kubernetes)
   - Брокер сообщений (RabbitMQ)

3. **Архитектурные диаграммы (1-2 мин)**
   - C4 диаграмма System Context (что в системе)
   - C4 диаграмма Container (какие контейнеры/сервисы)
   - Диаграмма компонентов (Bounded Contexts)
   - Диаграмма развёртывания (Docker, Kubernetes)

4. **Демонстрация работы (3-4 мин)**
   - Запуск `docker-compose up`
   - Открыть UI в браузере
   - Вход с токеном
   - Создание ордера
   - Запуск стратегии
   - Показать логи в Grafana/Prometheus

5. **Демонстрация паттернов (1 мин)**
   - Показать код: как реализован Circuit Breaker
   - Показать код: как реализована SAGA
   - Показать запрос в RabbitMQ

6. **Вопросы и ответы (1-2 мин)**

#### Что иметь готовым

**Обязательно**:
- [ ] Docker Compose файл, который запускает всё (`docker-compose up`)
- [ ] Postman коллекция со сценариями (как минимум)
- [ ] README с инструкциями по запуску
- [ ] 2-3 C4 диаграммы архитектуры
- [ ] Скриншоты UI (если есть)

**Рекомендуется**:
- [ ] Prometheus Dashboard
- [ ] Grafana с метриками
- [ ] Логи в ELK или просто в stdout
- [ ] Примеры кода основных паттернов

**Не забыть**:
- [ ] Проверить, что всё работает за день до защиты
- [ ] Подготовить ответы на типичные вопросы
- [ ] Записать demo скрипт (чтобы не срываться на живой демо)

---

## Блок 3: Паттерны микросервисной архитектуры

### 3.1 Архитектурные паттерны разложения (decomposition)

#### 3.1.1 Паттерн 1: Decompose by Business Capability

**Суть**: разбить систему по функциональным возможностям (capabilities) бизнеса.

**Пример для Trading Platform**:
- Market Data Service — получение котировок
- Trading Service — управление ордерами
- Portfolio Service — отслеживание позиций
- Strategy Service — торговые стратегии
- Notification Service — уведомления пользователю

**Преимущества**:
- Каждый сервис отвечает за одну область
- Команды могут работать независимо
- Легко масштабировать отдельный сервис

**Недостатки**:
- Может привести к дублированию кода между сервисами
- Сложнее отследить бизнес-транзакции, проходящие через несколько сервисов

#### 3.1.2 Паттерн 2: Decompose by Subdomain (DDD)

**Суть**: использовать Domain-Driven Design, разделить на субдомены (bounded contexts).

**Пример**:
- **Core Domain** — Trading (создание ордеров, стратегии) — конкурентное преимущество
- **Supporting Domain** — Market Data (котировки) — важно но не уникально
- **Generic Domain** — Identity (аутентификация) — можно использовать готовое решение

**Преимущества**:
- Каждый домен изолирован с его моделью данных
- Явное описание терминов (bounded context)
- Легче найти естественные границы для микросервисов

**Недостатки**:
- Требует понимания бизнеса
- Может не совпадать с организационной структурой

#### 3.1.3 Паттерн 3: Strangler Pattern

**Суть**: постепенная миграция монолита в микросервисы, старый и новый код работают параллельно.

**Как работает**:
1. API Gateway перенаправляет запросы: новые → новый микросервис, старые → старый монолит
2. Постепенно переносим функционал из монолита в микросервис
3. Удаляем код из монолита когда он полностью мигрирован

**Преимущества**:
- Может работать на production с минимальными рисками
- Позволяет тестировать новую архитектуру параллельно со старой

**Недостатки**:
- Сложно в реализации
- Нужно поддерживать оба варианта кода

---

### 3.2 Паттерны взаимодействия (communication patterns)

#### 3.2.1 Паттерн 4: Synchronous Request/Response (REST, gRPC)

**Суть**: сервис A отправляет запрос сервису B и ждёт ответа.

```
ServiceA: POST /api/v1/orders
   └─→ OrderService: создаёт заказ
      └─→ BrokerGateway: отправляет брокеру
         └─→ Tinkoff API: создаёт ордер
            └─ ответ
         └─ ответ
      └─ ответ
   └─ ответ
```

**Когда использовать**:
- Нужен сразу ответ (request-response)
- Сервис B быстро отвечает
- Хороший сетевой интерфейс

**Недостатки**:
- Если сервис B упадёт, A не сможет работать
- Медленные операции блокируют ресурсы

**Отказоустойчивость**: Circuit Breaker, Retry, Timeout

#### 3.2.2 Паттерн 5: Asynchronous Message-based (Event-Driven)

**Суть**: сервис публикует событие, а кто заинтересован — подписывается.

```
OrderService: публикует OrderCreated event в RabbitMQ
   └─ topics: "orders.created"
      ├─ StrategyService: слушает, если сигнал → создаёт ордер
      ├─ NotificationService: слушает, отправляет уведомление пользователю
      └─ AnalyticsService: слушает, обновляет статистику
```

**Когда использовать**:
- Асинхронная обработка (не нужен ответ сразу)
- Один event несколько подписчиков
- Расчёт на eventual consistency

**Преимущества**:
- Слабая связанность (сервисы не знают друг о друге)
- Хорошо масштабируется
- Отказоустойчив (если подписчик упадёт, событие сохранится в очереди)

**Недостатки**:
- Нельзя знать ошибку в реальном времени
- Нужна дополнительная infrastrucure (message broker)
- Отладка сложнее

#### 3.2.3 Паттерн 6: Orchestration vs Choreography

**Orchestration** (оркестрация):
- Центральный сервис (Orchestrator) управляет ходом процесса
- Orchestrator знает о других сервисах и их бизнес-логике

```
OrderOrchestrator:
  1. OrderService.createOrder()
  2. BrokerService.sendOrder()
  3. PaymentService.charge() (если нужно)
  4. NotificationService.notify()
```

Проблема: Orchestrator становится complex, знает слишком много бизнес-логики.

**Choreography** (хореография):
- Каждый сервис независим, реагирует на события других
- Нет центрального координатора

```
1. OrderService создаёт заказ и публикует OrderCreated
2. BrokerService слушает OrderCreated и отправляет заказ
3. BrokerService публикует OrderSent
4. NotificationService слушает OrderSent и уведомляет пользователя
```

Преимущество: каждый сервис фокусируется на своей логике.

**На практике**: часто используют комбинацию.

#### 3.2.4 Паттерн 7: Publish-Subscribe (Observer)

**Суть**: подписчики регистрируются на интересующие события.

**Реализация**:
- Message broker (RabbitMQ topic exchanges, Kafka topics)
- Event bus (pub-sub в процессе, для монолита)

**Пример**:
```cpp
// Subscribe
eventBus.subscribe("orders.created", [](const OrderCreatedEvent& event) {
    strategyService.handleOrderCreated(event);
});

// Publish
eventBus.publish(OrderCreatedEvent{...});
```

---

### 3.3 Паттерны для данных

#### 3.3.1 Паттерн 8: Database per Service

**Суть**: каждый микросервис имеет свою БД, не делится с другими.

**Преимущества**:
- Независимость: изменение схемы одного сервиса не влияет на другие
- Оптимизация: каждый сервис выбирает БД под свои потребности (PostgreSQL, NoSQL, etc.)
- Масштабирование: каждый сервис масштабируется независимо

**Недостатки**:
- Сложность консистентности между БД
- Нельзя делать join между таблицами разных сервисов

**Решение**: eventual consistency, SAGA паттерн, event sourcing.

#### 3.3.2 Паттерн 9: Event Sourcing

**Суть**: не хранить текущее состояние, а хранить все события (изменения).

**Пример**:
```
Order события:
1. OrderCreated(id=123, figi=SBER, qty=10, price=260)
2. OrderConfirmed(id=123, broker_id=456)
3. OrderPartiallyFilled(id=123, filled_qty=5)
4. OrderFilled(id=123)
```

Текущее состояние заказа = воспроизведение всех событий:
- После события 1: статус = PENDING
- После события 2: статус = CONFIRMED
- После события 3: статус = PARTIALLY_FILLED
- После события 4: статус = FILLED

**Преимущества**:
- Полная аудитория всех изменений
- Можно восстановить состояние на любой момент времени
- Event sourcing + CQRS = powerful combo

**Недостатки**:
- Сложность в реализации
- Требует migration при изменении формата событий

#### 3.3.3 Паттерн 10: CQRS (Command Query Responsibility Segregation)

**Суть**: разделить операции на две части:
- **Command** (write): изменение состояния
- **Query** (read): чтение состояния

Часто с разными моделями данных для каждого.

**Пример**:

```
Command side (write):
  OrderService.createOrder() → пишет в Order aggregate → Event published

Query side (read):
  OrderQueryService.getOrders() → читает из OrderView (denormalized) → быстро
```

**Когда полезно**:
- Разные pattern read/write (много читаем, мало пишем)
- Event Sourcing (events как source of truth для write, denormalized views для read)
- Масштабирование: read и write масштабируются независимо

**Когда не нужно**: для простых CRUD приложений может быть overkill.

#### 3.3.4 Паттерн 11: SAGA (распределённые транзакции)

**Суть**: разбиение одной большой транзакции на цепь локальных с возможностью компенсации.

**Пример**: заказ требует нескольких шагов:
```
1. OrderService: создать заказ (ORDER_PENDING)
2. BrokerService: отправить брокеру (ORDER_SENT)
3. PaymentService: снять деньги (PAID)
4. NotificationService: отправить уведомление (NOTIFIED)
```

Если на шаге 3 нет денег:
```
Компенсация:
1. OrderService: отменить заказ
2. BrokerService: отменить отправку
```

**Два типа**:

**Choreography (хореография)**:
- Каждый сервис слушает события и действует независимо
- Нет центрального координатора
- Проще в начале, сложнее отладить

**Orchestration (оркестрация)**:
- Есть Saga Orchestrator, который управляет ходом
- Явные шаги и откаты
- Проще отладить, но Orchestrator может стать bottleneck

#### 3.3.5 Паттерн 12: Outbox Pattern (надёжная публикация)

**Суть**: гарантировать, что событие будет опубликовано, даже если message broker упадёт.

**Как работает**:
```
1. BEGIN transaction
   INSERT INTO orders (...);
   INSERT INTO outbox (event_type, payload) VALUES ('OrderCreated', ...);
   COMMIT;

2. Отдельный процесс читает outbox и публикует события в RabbitMQ

3. После успешной публикации: DELETE FROM outbox WHERE id = ...;
```

**Гарантирует**: at-least-once delivery (событие может прийти несколько раз, нужна идемпотентность).

---

### 3.4 Паттерны для отказоустойчивости (resilience)

#### 3.4.1 Паттерн 13: Circuit Breaker

**Суть**: если сервис B часто падает, Circuit Breaker перестаёт обращаться к нему и быстро возвращает ошибку.

**Состояния**:
- **Closed** (нормально): запросы идут в сервис B
- **Open** (проблема): запросы к B идут напрямую в ошибку (fail-fast), не тратим время
- **Half-open** (восстановление): периодически пробуем запрос в B, если успешен → Closed

**Конфигурация**:
```
- Threshold: если 5 ошибок из 10 → открыть
- Timeout: сколько времени ждём ответ
- Half-open period: как часто проверяем восстановление
```

**Когда использовать**: всегда для внешних вызовов (Tinkoff API, БД).

#### 3.4.2 Паттерн 14: Retry with Exponential Backoff

**Суть**: повторить операцию с возрастающей задержкой.

**Пример**:
```
Попытка 1: сразу
Попытка 2: ждём 1сек
Попытка 3: ждём 2сек
Попытка 4: ждём 4сек
...
```

**Когда**: для временных ошибок (timeout, network glitch).

**Когда НЕ**: для постоянных ошибок (ошибка авторизации не решится пересчётом).

#### 3.4.3 Паттерн 15: Timeout

**Суть**: установить максимальное время ожидания ответа.

```cpp
// Если Tinkoff API не ответит за 5 сек → ошибка
timeout(5s) {
    tinkoffApi.getQuote(figi);
}
```

**Почему**: без timeout операция может зависнуть навсегда (ресурсы утекут).

---

### 3.5 Паттерны для масштабирования

#### 3.4.1 Паттерн 16: Service Discovery

**Суть**: сервисы динамически регистрируются и находятся друг друга.

**Как работает**:
```
1. OrderService стартует → регистрируется в Service Registry с адресом
2. Strategy Service нужно обращаться к OrderService
3. Strategy Service запрашивает у Service Registry адрес OrderService
4. Получает список адресов (может быть несколько инстансов)
5. Обращается к одному из них
```

**Инструменты**:
- Kubernetes DNS (встроено, сервис = DNS имя)
- Consul
- Eureka (Spring Cloud)

#### 3.4.2 Паттерн 17: Load Balancing

**Суть**: распределение нагрузки между несколькими инстансами сервиса.

**Стратегии**:
- Round-robin: по очереди
- Least connections: к тому с меньше активных соединений
- Random
- Weighted (некоторые сервера мощнее)

**Место**: часто в Service Mesh (Istio) или API Gateway.

#### 3.4.3 Паттерн 18: Rate Limiting

**Суть**: ограничить количество запросов в единицу времени.

**Где**:
- На API Gateway: max X запросов от одного пользователя
- К внешним API: уважение к лимитам Tinkoff (например, 100 запросов в сек)

---

### 3.6 Паттерны мониторинга

#### 3.6.1 Паттерн 19: Structured Logging

**Суть**: логировать в структурированном формате (JSON), не просто текст.

**Почему**: легче парсить, агрегировать и анализировать логи в ELK.

**Пример плохого**:
```
2025-12-12 14:30:45 Created order for user 123
```

**Пример хорошего**:
```json
{
  "timestamp": "2025-12-12T14:30:45Z",
  "level": "INFO",
  "service": "order-service",
  "event": "order.created",
  "user_id": 123,
  "order_id": "uuid-456",
  "figi": "BBG004730N88",
  "quantity": 10,
  "trace_id": "trace-789"
}
```

**С trace_id**: можно отследить один запрос через все сервисы.

#### 3.6.2 Паттерн 20: Distributed Tracing

**Суть**: отследить один request через все микросервисы.

**Как работает**:
```
1. UI отправляет запрос: POST /api/v1/orders с header X-Trace-ID: abc123
2. API Gateway → OrderService (пробрасывает trace_id)
3. OrderService → BrokerGateway (пробрасывает trace_id)
4. BrokerGateway → Tinkoff API (пробрасывает trace_id)

Результат: видим весь путь запроса (latency на каждом шаге)
```

**Инструменты**: Jaeger, Zipkin, OpenTelemetry.

---

### 3.7 Обновления и развёртывание

#### 3.7.1 Паттерн 21: Blue-Green Deployment

**Суть**: два окружения (Blue и Green), переключаемся между ними.

```
Production (Blue) — версия v1 (пользователи работают)
Staging (Green) — версия v2 (тестируем)

После тестирования: переключаем трафик на Green → она становится Blue
Если проблемы → быстро переключаемся обратно
```

**Преимущества**:
- Быстрый откат
- Минимальный downtime (instant switch)

**Недостатки**:
- Нужно два окружения (дороже)
- Нужна синхронизация БД

#### 3.7.2 Паттерн 22: Canary Deployment

**Суть**: новая версия постепенно получает всё больше трафика.

```
v1: 95% трафика
v2: 5% трафика

Если ошибок в v2 < threshold:
v1: 90%
v2: 10%
...
Пока v2 не будет получать 100% трафика
```

**Преимущества**:
- Риск минимален (только 5% пользователей видит новую версию)
- Быстрое обнаружение проблем

**Инструменты**: Kubernetes (Istio может помочь), управление трафиком в API Gateway.

#### 3.7.3 Паттерн 23: Rolling Deployment

**Суть**: постепенное обновление инстансов.

```
3 инстанса сервиса:
1. Останови инстанс 1, обнови, запусти (остаток 2 работает)
2. Останови инстанс 2, обнови, запусти (остаток 2 работает)
3. Останови инстанс 3, обнови, запусти (все работают)
```

**Дефолт в Kubernetes**, очень надёжный способ.

---

### 3.8 Сравнительная таблица основных паттернов

| # | Паттерн | Когда использовать | Альтернатива |
|---|---------|-------------------|-------------|
| 1 | Decompose by Capability | Функциональное разбиение | DDD подход |
| 2 | Decompose by Subdomain (DDD) | Бизнес-ориентированное разбиение | Capability-based |
| 3 | Strangler | Миграция монолита → микросервисы | Big Bang rewrite |
| 4 | Request/Response (REST) | Синхронные операции с ответом | gRPC, GraphQL |
| 5 | Event-Driven | Асинхронное взаимодействие, слабая связанность | Message Queue (но это же) |
| 6 | Choreography | Сложные workflow без центра | Orchestration |
| 7 | Orchestration | Явная логика workflow | Choreography |
| 8 | Database per Service | Независимость БД сервисов | Shared database (монолит) |
| 9 | Event Sourcing | Полная история изменений | Снимки состояния |
| 10 | CQRS | Разные паттерны read/write | Единая модель (монолит) |
| 11 | SAGA | Распределённые транзакции | 2PC (медленно) |
| 12 | Outbox | Надёжная публикация событий | Отправка сразу (может потеряться) |
| 13 | Circuit Breaker | Отказоустойчивость к сбоям сервиса | Timeout только |
| 14 | Retry with Backoff | Транзиентные ошибки | Сразу ошибка пользователю |
| 15 | Timeout | Предотвращение зависания | Без таймаута |
| 16 | Service Discovery | Динамическое поиск сервисов | Статические IP адреса |
| 17 | Load Balancing | Масштабирование, высокая доступность | Single instance |
| 18 | Rate Limiting | Защита от DDoS, соблюдение лимитов API | Без ограничений |
| 19 | Structured Logging | Анализ логов в ELK | Текстовые логи |
| 20 | Distributed Tracing | Отладка в микросервисной среде | Логи от каждого сервиса |
| 21 | Blue-Green Deployment | Минимальный downtime, быстрый откат | Rolling deployment |
| 22 | Canary Deployment | Минимизация риска новых версий | Blue-Green |
| 23 | Rolling Deployment | Стандартное обновление | Blue-Green, Canary |

---

## Заключение

Защита проекта — это не просто демонстрация кода, а показ **архитектурного мышления**. Преподаватели проверяют:

1. **Знаете ли вы паттерны** и когда их применять
2. **Понимаете ли вы trade-offs** (каждое решение имеет цену)
3. **Сможете ли вы масштабировать** систему в будущем
4. **Можете ли вы делать informed decisions** о технологиях

**На защиту приготовьте**:
- ✅ Работающий MVP (docker-compose up)
- ✅ Архитектурные диаграммы (C4)
- ✅ Постман коллекцию или UI
- ✅ Примеры реализованных паттернов в коде
- ✅ Ответы на типичные вопросы (декомпозиция, мониторинг, консистентность)
- ✅ Размышления о масштабировании (как бы вы развернули как микросервисы)

**Помните**: лучше реализовать 3 паттерна хорошо, чем 10 паттернов плохо. Фокус на качество!